{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CMU-MOSI Multimodal SDK"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download and Process mosi"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### word level align"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import mmsdk\n",
    "from mmsdk import mmdatasdk\n",
    "import numpy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#A simple averaging technique. More advanced methods can be built based on intervals.\n",
    "def myavg(intervals,features):\n",
    "        return numpy.average(features,axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Downloading the dataset\n",
    "cmumosi_highlevel=mmdatasdk.mmdataset(mmdatasdk.cmu_mosi.highlevel,'cmumosi/')\n",
    "# cmumosi_highlevel=mmdatasdk.mmdataset('cmumosi/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#some random video from cmumosi_highlevel\n",
    "some_video=list(cmumosi_highlevel[\"glove_vectors\"].data.keys())[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#The next line is not needed for standard datasets as they are all sorted based on intervals in computational sequence entries\n",
    "cmumosi_highlevel.sort()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Aligning to the words to get word-level alignments\n",
    "cmumosi_highlevel.align('glove_vectors',collapse_functions=[myavg])\n",
    "cmumosi_highlevel.impute('glove_vectors')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#get the intervals and features accompanying the 100th word in the some_video\n",
    "some_video_100th_word=some_video+'[100]'\n",
    "for compseq_name in list(cmumosi_highlevel.computational_sequences.keys()):\n",
    "\tcompseq=cmumosi_highlevel[compseq_name]\n",
    "\tprint (compseq_name)\n",
    "\tprint (numpy.array(compseq.data[some_video_100th_word][\"intervals\"]).shape,numpy.array(compseq.data[some_video_100th_word][\"features\"]).shape)\n",
    "\tprint (\"-------\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "glove_vectors\n",
      "(1, 2) (1, 300)\n",
      "-------\n",
      "FACET_4.1\n",
      "(1, 2) (1, 47)\n",
      "-------\n",
      "FACET_4.2\n",
      "(1, 2) (1, 35)\n",
      "-------\n",
      "OpenSmile-emobase2010\n",
      "(1, 2) (1, 1585)\n",
      "-------\n",
      "OpenSMILE\n",
      "(1, 2) (1, 384)\n",
      "-------\n",
      "OpenFace_1\n",
      "(1, 2) (1, 430)\n",
      "-------\n",
      "OpenFace_2\n",
      "(1, 2) (1, 713)\n",
      "-------\n",
      "COVAREP\n",
      "(1, 2) (1, 74)\n",
      "-------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "k번째 비디오의 100번째 단어의 computational sequence를 관찰한 결과,\n",
    "\n",
    "* Text - glove_vectors (feature: 300)\n",
    "\n",
    "* Image - OpenFace_1 (430), FACET_4.2 (35), FACET_4.1 (47), COVAREP(74), OpenFace_2 (713)\n",
    "\n",
    "* Audio - OpenSmile_emobase2010 (1585), b'OpenSMILE (384)\n",
    "\n",
    "이를 통해 embedding feature 수 파악 가능.\n",
    "\n",
    "*참고로 k번째 비디오는 glove_vector로 리스트업 하여 추출한 것.*\n",
    "\n",
    "```python\n",
    "#some random video from cmumosi_highlevel\n",
    "some_video=list(cmumosi_highlevel[\"glove_vectors\"].data.keys())[0]\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![computational sequence](examples/compseq.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2번째 비디오를 관찰해보아도 마찬가지의 결과"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#get the intervals and features accompanying the 2nd in some_video\n",
    "some_video_2nd_segment=some_video+'[2]'\n",
    "for compseq_name in list(cmumosi_highlevel.computational_sequences.keys()):\n",
    "\tcompseq=cmumosi_highlevel[compseq_name]\n",
    "\tprint (compseq_name)\n",
    "\tprint (numpy.array(compseq.data[some_video_2nd_segment][\"intervals\"]).shape,numpy.array(compseq.data[some_video_2nd_segment][\"features\"]).shape)\n",
    "\tprint (\"-------\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "glove_vectors\n",
      "(1, 2) (1, 300)\n",
      "-------\n",
      "FACET_4.1\n",
      "(1, 2) (1, 47)\n",
      "-------\n",
      "FACET_4.2\n",
      "(1, 2) (1, 35)\n",
      "-------\n",
      "OpenSmile-emobase2010\n",
      "(1, 2) (1, 1585)\n",
      "-------\n",
      "OpenSMILE\n",
      "(1, 2) (1, 384)\n",
      "-------\n",
      "OpenFace_1\n",
      "(1, 2) (1, 430)\n",
      "-------\n",
      "OpenFace_2\n",
      "(1, 2) (1, 713)\n",
      "-------\n",
      "COVAREP\n",
      "(1, 2) (1, 74)\n",
      "-------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#Deploying the files to the disk and reading them again - Building machine learning models start right after this. No need to do alignment multiple times since aligned files can be deployed and used again.\n",
    "# deploy_files={x:x for x in cmumosi_highlevel.computational_sequences.keys()}\n",
    "# cmumosi_highlevel.deploy(\"./deployed\",deploy_files)\n",
    "\n",
    "# #Reading the dumped file can be as easy as just calling the mmdataset on the deployed folder\n",
    "# aligned_cmumosi_highlevel=mmdatasdk.mmdataset('./deployed')\n",
    "\n",
    "#Now let's get the tensor ready for ML - right here we just get everything into ML ready tensors. But you should split the aligned_cmumosi_highlevel based on the standard CMU MOSI folds\n",
    "#get the standard folds using mmsdk.mmdatasdk.cmu_mosi.standard_folds.standard_x_fold for x={train,test,valid}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:301310)",
      "at w.execute (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:300703)",
      "at w.start (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:296367)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:311160)",
      "at async t.CellExecutionQueue.start (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:310700)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tensors=cmumosi_highlevel.get_tensors(seq_len=25,non_sequences=[\"Opinion Segment Labels\"],direction=False,folds=[mmdatasdk.cmu_mosi.standard_folds.standard_train_fold,mmdatasdk.cmu_mosi.standard_folds.standard_valid_fold,mmdatasdk.cmu_mosi.standard_folds.standard_test_fold])\n",
    "\n",
    "fold_names=[\"train\",\"valid\",\"test\"]\n",
    "\n",
    "for i in range(3):\n",
    "\tfor csd in list(cmumosi_highlevel.keys()):\n",
    "\t\tprint (\"Shape of the %s computational sequence for %s fold is %s\"%(csd,fold_names[i],tensors[i][csd].shape))\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:301310)",
      "at w.execute (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:300703)",
      "at w.start (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:296367)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:311160)",
      "at async t.CellExecutionQueue.start (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:310700)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}